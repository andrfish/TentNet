{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "43xdrtrhrmTC",
        "Th9xEy39xt0M",
        "CoOmuSRiI39e",
        "8P742qU927HQ",
        "FhPfzMEOG5XS",
        "Ls43Rw1g9R85",
        "NE7lS2FYILP1",
        "7dvkeAAN-8Ph"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxLvFFWohRK7"
      },
      "source": [
        "# **Codebase for [TentNet](https://ieeexplore.ieee.org/abstract/document/9283377)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYzKlld1hl43"
      },
      "source": [
        "In this notebook I will demonstrate three pre-trained convolutional neural network (CNN) models, created from scratch, on the following two datasets:\n",
        "\n",
        "\n",
        "1.   https://www.kaggle.com/rhammell/planesnet\n",
        "2.   https://www.kaggle.com/rhammell/ships-in-satellite-imagery\n",
        "\n",
        "In addition to this, I will demonstrate theses models on two different synthetic datasets that will be generated by a generative adversarial network (GAN) model from NVIDIA and TensorFlow respectively. This is for the main purpose of this project: identifying tents in urban areas from satellite imagery.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7bimpER9Gyr"
      },
      "source": [
        "**Please note that a large portion of this code comes from the first deliverable as I am presenting the entire project now.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-6b9GE9mIN9"
      },
      "source": [
        "> Use these flags to first define the environment this notebook is being ran on\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsJVtRIdnIHQ"
      },
      "source": [
        "# Set what I'm doing with this instance of the notebook\n",
        "# For some reason, the GAN code and the CNN code cannot be ran in the same\n",
        "# instance due to the GAN code requiring \"eager execution\" while the CNN\n",
        "# code requires that not be the case\n",
        "run_cnn = False\n",
        "\n",
        "# Set what I'm running this notebook on\n",
        "colab = True\n",
        "\n",
        "# Decide to use CPU instead of GPU if desired\n",
        "use_cpu = False\n",
        "cpu_cores = 4\n",
        "cpu_count = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-VsHuR_EI6r"
      },
      "source": [
        "> Then run this cell to take care of some initial imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G96gVISiEI6t"
      },
      "source": [
        "# Install a package called 'pillow' to import this\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX7LblWqrKpM"
      },
      "source": [
        "# Dataset Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8jLLV1MiVW2"
      },
      "source": [
        "\n",
        "\n",
        "> First, let's extract the datasets assuming that they're named as \"planes.zip\" and \"ships.zip\", respectively, from the following two links:\n",
        "\n",
        "\n",
        "1.   https://www.kaggle.com/rhammell/planesnet\n",
        "2.   https://www.kaggle.com/rhammell/ships-in-satellite-imagery\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19CYikPCmFDl"
      },
      "source": [
        "!unzip -q planes.zip -d planes\n",
        "!unzip -q ships.zip -d ships"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "res5K2OlG5Vd"
      },
      "source": [
        "> Next, let's load these datasets with the json library\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdO6EOktG5Ve"
      },
      "source": [
        "# Load the planes dataset's json\n",
        "path_preceed = '/content/' if colab else ''\n",
        "with open(path_preceed + 'planes/planesnet.json') as dataset:\n",
        "    planes_dataset = json.load(dataset)\n",
        "\n",
        "# Load the ship dataset's json\n",
        "with open(path_preceed + 'ships/shipsnet.json') as dataset:\n",
        "    ships_dataset = json.load(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkL2JKLUqHbC"
      },
      "source": [
        "\n",
        "\n",
        "> Finally, let's view an image from each dataset using sample code from the datasets' authors on Kaggle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI8rywo0pWS7"
      },
      "source": [
        "# First, show a plane\n",
        "# https://www.kaggle.com/rhammell/displaying-and-saving-image-chips\n",
        "plane = np.array(planes_dataset['data'][22]).astype('uint8')\n",
        "plane = plane.reshape((3, 400)).T.reshape((20, 20, 3))\n",
        "plt.imshow(plane)\n",
        "print(\"Here's an example plane from the dataset:\")\n",
        "plt.show()\n",
        "\n",
        "# Second, show a ship\n",
        "# https://www.kaggle.com/rhammell/reforming-and-displaying-image-chips\n",
        "ship = np.array(ships_dataset['data'][22]).astype('uint8')\n",
        "ship = ship.reshape((3, 6400)).T.reshape((80, 80, 3))\n",
        "plt.imshow(ship)\n",
        "print(\"\\nHere's an example ship from the dataset:\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43xdrtrhrmTC"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpO3SEScq9yR"
      },
      "source": [
        "\n",
        "\n",
        "> First, let's import the libraries we'll need to make the CNN models and set the seed\n",
        "*   The code for setting the seeds came from the Keras documentation here: https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUxr0ZRcry3A"
      },
      "source": [
        "# Hide warnings and logging\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.CRITICAL)\n",
        "\n",
        "if colab:\n",
        "  %tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "if not run_cnn:\n",
        "  tf.enable_eager_execution()\n",
        "  tf.config.experimental_run_functions_eagerly(True)\n",
        "\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Import the modules needed from Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import LeakyReLU, ReLU, PReLU, Softmax, ZeroPadding2D\n",
        "from keras.optimizers import Adagrad, Adam, SGD, RMSprop, Nadam\n",
        "from keras import regularizers \n",
        "\n",
        "# Set the seeds\n",
        "import random as rn\n",
        "np.random.seed(2020)\n",
        "rn.seed(2020)\n",
        "tf.set_random_seed(2020)\n",
        "\n",
        "# Set the tensorflow configuration\n",
        "from keras import backend as K\n",
        "from keras.backend.tensorflow_backend import set_session  \n",
        "config = tf.ConfigProto(log_device_placement=True)\n",
        "\n",
        "# Set which device to use\n",
        "if use_cpu:\n",
        "    import os\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "\n",
        "    config = tf.ConfigProto(\n",
        "        intra_op_parallelism_threads=cpu_cores,\n",
        "        inter_op_parallelism_threads=cpu_cores, \n",
        "        allow_soft_placement=True,\n",
        "        device_count = {'CPU' : cpu_count,\n",
        "                        'GPU' : 0},\n",
        "        log_device_placement=True\n",
        "    )\n",
        "else:\n",
        "    config = tf.ConfigProto(\n",
        "        log_device_placement=True\n",
        "    )\n",
        "    \n",
        "    # Optimize my personal GPU if not on Colab\n",
        "    if not colab:\n",
        "        config.gpu_options.allow_growth=True\n",
        "\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "if run_cnn:\n",
        "  K.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bdC00G1ss6w"
      },
      "source": [
        "\n",
        "\n",
        "> **Next, let's define the method to create the first model**\n",
        "*   This transfers learning from the ResNet50V2 model in Keras\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eE_z0PWtDhI"
      },
      "source": [
        "from keras.applications.resnet_v2 import ResNet50V2\n",
        "\n",
        "def create_cnn_model_one(width, height, depth, outputs = 2, \n",
        "                     optimizer = Adagrad, activation = ReLU, \n",
        "                     learning_rate = 0.001):\n",
        "    # Initialize the model\n",
        "    model = Sequential()\n",
        "    pre_trained = ResNet50V2(weights='imagenet', include_top=False,\n",
        "                                    input_shape=(width, height, depth))\n",
        "\n",
        "    # Add a flatten layer\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Add a dense layer\n",
        "    model.add(Dense(256,\n",
        "                    activation = 'relu',\n",
        "                    kernel_initializer='random_normal'))\n",
        "    \n",
        "    # Add a dropout layer\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Add a batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Add a dense layer\n",
        "    model.add(Dense(128,\n",
        "                    activation = 'relu',\n",
        "                    kernel_initializer='random_normal'))\n",
        "    \n",
        "    # Add a dropout layer\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    # Add a batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Add a dense layer\n",
        "    model.add(Dense(64,\n",
        "                    activation = 'relu',\n",
        "                    kernel_initializer='random_normal'))\n",
        "    \n",
        "    # Add a dropout layer\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    # Add a batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Add a dense layer\n",
        "    model.add(Dense(16,\n",
        "                    activation = 'relu',\n",
        "                    kernel_initializer='random_normal'))\n",
        "\n",
        "    # Add a batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Finally, add the output layer\n",
        "    model.add(Dense(outputs,\n",
        "                    activation = 'softmax',\n",
        "                    kernel_initializer='random_normal'))\n",
        "\n",
        "    # Compile the model\n",
        "    complete_model = Sequential()\n",
        "    complete_model.add(pre_trained)\n",
        "    complete_model.add(model)\n",
        "    complete_model.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = optimizer(lr=learning_rate),\n",
        "                  metrics = ['accuracy'])\n",
        "\n",
        "    complete_model.name = \"TentNetwork_ResNet50V2\"\n",
        "    return complete_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NstrxgzD_XlE"
      },
      "source": [
        "\n",
        "\n",
        "> **Next, let's define the method to create the second model**\n",
        "*   This transfers learning from the InceptionV3 model in Keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O8SB9cm_XlK"
      },
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "def create_cnn_model_two(width, height, depth, outputs = 2, \n",
        "                     optimizer = Adagrad, activation = ReLU, \n",
        "                     learning_rate = 0.001):\n",
        "    # Initialize the model\n",
        "    model = Sequential()\n",
        "    pre_trained = InceptionV3(weights='imagenet', include_top=False,\n",
        "                              input_shape=(width, height, depth))\n",
        "\n",
        "    # Add a flatten layer\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Add a dense layer\n",
        "    model.add(Dense(256,\n",
        "                    activation = 'relu',\n",
        "                    kernel_initializer='random_normal'))\n",
        "    \n",
        "    # Add a dropout layer\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Add a batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Add a dense layer\n",
        "    model.add(Dense(128,\n",
        "                    activation = 'relu',\n",
        "                    kernel_initializer='random_normal'))\n",
        "    \n",
        "    # Add a dropout layer\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    # Add a batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Add a dense layer\n",
        "    model.add(Dense(64,\n",
        "                    activation = 'relu',\n",
        "                    kernel_initializer='random_normal'))\n",
        "    \n",
        "    # Add a dropout layer\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    # Add a batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Add a dense layer\n",
        "    model.add(Dense(16,\n",
        "                    activation = 'relu',\n",
        "                    kernel_initializer='random_normal'))\n",
        "\n",
        "    # Add a batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Finally, add the output layer\n",
        "    model.add(Dense(outputs,\n",
        "                    activation = 'softmax',\n",
        "                    kernel_initializer='random_normal'))\n",
        "\n",
        "    # Compile the model\n",
        "    complete_model = Sequential()\n",
        "    complete_model.add(pre_trained)\n",
        "    complete_model.add(model)\n",
        "    complete_model.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = optimizer(lr=learning_rate),\n",
        "                  metrics = ['accuracy'])\n",
        "\n",
        "    complete_model.name = \"TentNetwork_InceptionV3\"\n",
        "    return complete_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQYlw4ra_awD"
      },
      "source": [
        "\n",
        "\n",
        "> **Next, let's define the method to create the third model**\n",
        "*   This transfers learning from the MobileNet2 model in Keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kma5G4c2_awF"
      },
      "source": [
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "\n",
        "def create_cnn_model_three(width, height, depth, outputs = 2, \n",
        "                     optimizer = Adagrad, activation = ReLU, \n",
        "                     learning_rate = 0.001):\n",
        "    # Initialize the model\n",
        "    model = Sequential()\n",
        "    pre_trained = MobileNetV2(weights='imagenet', include_top=False,\n",
        "                              input_shape=(width, height, depth))\n",
        "\n",
        "    # Add a flatten layer\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Add a dense layer\n",
        "    model.add(Dense(256,\n",
        "                    activation = 'relu',\n",
        "                    kernel_initializer='random_normal'))\n",
        "    \n",
        "    # Add a dropout layer\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Add a batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Add a dense layer\n",
        "    model.add(Dense(128,\n",
        "                    activation = 'relu',\n",
        "                    kernel_initializer='random_normal'))\n",
        "    \n",
        "    # Add a dropout layer\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    # Add a batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Add a dense layer\n",
        "    model.add(Dense(64,\n",
        "                    activation = 'relu',\n",
        "                    kernel_initializer='random_normal'))\n",
        "    \n",
        "    # Add a dropout layer\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    # Add a batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Add a dense layer\n",
        "    model.add(Dense(16,\n",
        "                    activation = 'relu',\n",
        "                    kernel_initializer='random_normal'))\n",
        "\n",
        "    # Add a batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add activation layer\n",
        "    model.add(activation())\n",
        "\n",
        "    # Finally, add the output layer\n",
        "    model.add(Dense(outputs,\n",
        "                    activation = 'softmax',\n",
        "                    kernel_initializer='random_normal'))\n",
        "\n",
        "    # Compile the model\n",
        "    complete_model = Sequential()\n",
        "    complete_model.add(pre_trained)\n",
        "    complete_model.add(model)\n",
        "    complete_model.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = optimizer(lr=learning_rate),\n",
        "                  metrics = ['accuracy'])\n",
        "\n",
        "    complete_model.name = \"TentNetwork_MobileNetV2\"\n",
        "    return complete_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLiFgM707uVu"
      },
      "source": [
        "> Next, we will define a method that uses cross-fold validation to evaluate the model using data augmentation (code from the Keras documentation) and return the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzcYsNDk9uXd"
      },
      "source": [
        "import time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "kf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 2020)\n",
        "\n",
        "def validate_model(folds, inputs, outputs, outputs_indiv, params = None, epochs = 20, \n",
        "                   verbose=True, create_cnn_model = None):\n",
        "    # Initialize variables to keep track of the results\n",
        "    overall_train_acc = 0\n",
        "    overall_train_loss = 0\n",
        "    overall_test_acc = 0\n",
        "    overall_test_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    # Cycle through the folds\n",
        "    total_start = time.time()\n",
        "    for train_index, test_index in folds:\n",
        "        start = time.time()\n",
        "\n",
        "        count += 1\n",
        "        print(\"\\nEvaluating fold \" + str(count) + \"/\" + str(len(folds)), end=\"\")\n",
        "\n",
        "        # Split the data\n",
        "        X_train, X_test = inputs[train_index], inputs[test_index]\n",
        "        Y_train, Y_test = outputs[train_index], outputs[test_index]\n",
        "\n",
        "        # Initialize a model\n",
        "        if params == None:\n",
        "            model = create_cnn_model(len(inputs[0]),\n",
        "                                  len(inputs[0][0]),\n",
        "                                  len(inputs[0][0][0]))\n",
        "        else:\n",
        "            model = create_cnn_model(**params)\n",
        "\n",
        "        # Compute the class weights\n",
        "        weights = class_weight.compute_class_weight('balanced',\n",
        "                                                    np.unique(outputs_indiv[train_index]), \n",
        "                                                    outputs_indiv[train_index])\n",
        "\n",
        "        # Train and test the model on this fold\n",
        "        result = model.fit(X_train, Y_train,\n",
        "                           batch_size = 8,\n",
        "                           epochs = epochs,\n",
        "                           validation_data = (X_test, Y_test),\n",
        "                           class_weight = weights,\n",
        "                           verbose = 0)\n",
        "\n",
        "        # Store the accuracy and loss\n",
        "        overall_train_acc += result.history['acc'][len(result.history['acc']) - 1]\n",
        "        overall_train_loss += result.history['loss'][len(result.history['loss']) - 1]\n",
        "        overall_test_acc += result.history['val_acc'][len(result.history['val_acc']) - 1]\n",
        "        overall_test_loss += result.history['val_loss'][len(result.history['val_loss']) - 1]\n",
        "\n",
        "        # Print this fold's results if verbose\n",
        "        end = time.time()\n",
        "        if(verbose):\n",
        "            print(\": completed in \" + str(end - start) + \"s\" + \n",
        "                \"\\n\\tTrain accuracy\\t= \" + str(result.history['acc'][len(result.history['acc']) - 1]) + \n",
        "                \"\\n\\tTrain loss\\t= \" + str(result.history['loss'][len(result.history['loss']) - 1]) +\n",
        "                \"\\n\\tTest accuracy\\t= \" + str(result.history['val_acc'][len(result.history['val_acc']) - 1]) +\n",
        "                \"\\n\\tTest loss\\t= \" + str(result.history['val_loss'][len(result.history['val_loss']) - 1]))\n",
        "  \n",
        "    # Output and return the result\n",
        "    overall_train_acc /= count\n",
        "    overall_train_loss /= count\n",
        "    overall_test_acc /= count\n",
        "    overall_test_loss /= count\n",
        "\n",
        "    total_end = time.time()\n",
        "    print(\"\\nEvaluation completed in \" + str(total_end - total_start) + \"s\" + \n",
        "          \"\\n\\tTrain accuracy\\t= \" + str(overall_train_acc) + \n",
        "          \"\\n\\tTrain loss\\t= \" + str(overall_train_loss) +\n",
        "          \"\\n\\tTest accuracy\\t= \" + str(overall_test_acc) +\n",
        "          \"\\n\\tTest loss\\t= \" + str(overall_test_loss))\n",
        "\n",
        "    return {'train_acc' : overall_train_acc, \n",
        "          'train_loss' : overall_train_loss, \n",
        "          'test_acc' : overall_test_acc, \n",
        "          'test_loss' : overall_test_loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeZV6QlV7P8s"
      },
      "source": [
        "> Lastly, let's define a method to perform a grid search using sklearn on this model to determine the best parameters for each dataset before running the validation method previously defined\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T3W6dmO7YLN"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "def model_grid_search(classifier, grid, inputs, outputs):\n",
        "    grid = GridSearchCV(  estimator = classifier,\n",
        "                          param_grid = grid,\n",
        "                          n_jobs = -1 if colab else \n",
        "                                        (1 if not use_cpu else \n",
        "                                             (2 if cpu_count == 1 else \n",
        "                                                  cpu_cores)),\n",
        "                          cv = 2,\n",
        "                          verbose = 1)\n",
        "    result = grid.fit(inputs, outputs)\n",
        "    return result.best_score_, result.best_params_, grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th9xEy39xt0M"
      },
      "source": [
        "# Model Evaluation: Planes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa95cTyc0ezb"
      },
      "source": [
        "\n",
        "\n",
        "> First, let's prepare the dataset by converting each of the images to the author's suggestion as shown before\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek0xiPAg2nC0"
      },
      "source": [
        "# Initialize a list for the converted \n",
        "converted_dataset = list()\n",
        "\n",
        "# Cycle through each instance in the dataset\n",
        "for plane in planes_dataset['data']:\n",
        "    plane = np.array(plane).astype('uint8')\n",
        "    plane = plane.reshape((3, 400)).T.reshape((20, 20, 3))\n",
        "\n",
        "    # We need to pad the image to make it at least 75 x 75 which is the minmum\n",
        "    # size for the pre-trained models (makes it 80 x 80)\n",
        "    plane = np.pad(plane, ((30, 30), (30, 30), (0, 0)),\n",
        "                   mode='constant', constant_values=3)\n",
        "\n",
        "    converted_dataset.append(plane)\n",
        "\n",
        "# Convert the final list into a numpy array\n",
        "converted_dataset = np.asarray(converted_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcC5YL9c5t7X"
      },
      "source": [
        "> Next, let's get the outputs and split our dataset using the StratifiedKFold object defined before, and convert the outputs to onehot encoding using Keras' to_categorical method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a-hYP5C33my"
      },
      "source": [
        "# Split the dataset into 10 folds and store the indicies\n",
        "outputs = np.array(planes_dataset['labels']).astype('uint8')\n",
        "indicies = list(kf.split(converted_dataset, outputs))\n",
        "\n",
        "# Convert the outputs to onehot encoding\n",
        "from keras.utils import to_categorical\n",
        "outputs_onehot = to_categorical(outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujeBT2vuC4ak"
      },
      "source": [
        "---\n",
        "**Model 1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQMKc3aD7myG"
      },
      "source": [
        "> Next, let's run a grid search on the model to determine the best parameters for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H2W0vTl3blY"
      },
      "source": [
        "# Define the model\n",
        "classifier = KerasClassifier(build_fn=create_cnn_model_one, verbose=0)\n",
        "\n",
        "# Define the parameters\n",
        "width = [plane.shape[0]]\n",
        "height = [plane.shape[1]]\n",
        "depth = [plane.shape[2]]\n",
        "learning_rates = [0.001]\n",
        "activations = [ReLU, Softmax]\n",
        "optimizers = [Adagrad, Adam]\n",
        "\n",
        "# Turn it into a dictionary for the grid\n",
        "grid = dict(width = width,\n",
        "            height = height,\n",
        "            depth = depth,\n",
        "            learning_rate = learning_rates,\n",
        "            activation = activations,\n",
        "            optimizer = optimizers)\n",
        "\n",
        "# Run the grid search\n",
        "best_score_one, best_params_one, _ = model_grid_search(classifier, grid, \n",
        "                                             converted_dataset, outputs_onehot)\n",
        "print(\"Best score: \" + str(best_score_one))\n",
        "print(\"Best parameters: \" + str(best_params_one))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kddjdeyPG5WT"
      },
      "source": [
        "> Next, let's see what the best model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQeu7S_-G5WV"
      },
      "source": [
        "# Initialize the best model\n",
        "best_params_one = {'width': plane.shape[0], 'height': plane.shape[1], 'depth': plane.shape[2],\n",
        "                   'learning_rate': 0.001, 'activation': ReLU, 'optimizer': Adam}\n",
        "model_one = create_cnn_model_one(**best_params_one)\n",
        "model_one.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbIi-0NH3c5D"
      },
      "source": [
        "> Finally, let's validate this best accuracy using the method defined before with data augmentation and 10-cross fold validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57Yq4Xdd7l9P"
      },
      "source": [
        "results_one = validate_model(indicies,\n",
        "                             converted_dataset, outputs_onehot, outputs,\n",
        "                             best_params_one,\n",
        "                             create_cnn_model = create_cnn_model_one)\n",
        "print(results_one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRxukDuPDPms"
      },
      "source": [
        "---\n",
        "**Model 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA1i_mBSDXzM"
      },
      "source": [
        "> Next, let's run a grid search on the model to determine the best parameters for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_96GxjHUDXzQ"
      },
      "source": [
        "# Define the model\n",
        "classifier = KerasClassifier(build_fn=create_cnn_model_two, verbose=0)\n",
        "\n",
        "# Define the parameters\n",
        "width = [plane.shape[0]]\n",
        "height = [plane.shape[1]]\n",
        "depth = [plane.shape[2]]\n",
        "learning_rates = [0.001]\n",
        "activations = [ReLU, Softmax]\n",
        "optimizers = [Adagrad, Adam]\n",
        "\n",
        "# Turn it into a dictionary for the grid\n",
        "grid = dict(width = width,\n",
        "            height = height,\n",
        "            depth = depth,\n",
        "            learning_rate = learning_rates,\n",
        "            activation = activations,\n",
        "            optimizer = optimizers)\n",
        "\n",
        "# Run the grid search\n",
        "best_score_two, best_params_two, _ = model_grid_search(classifier, grid, \n",
        "                                             converted_dataset, outputs_onehot)\n",
        "print(\"Best score: \" + str(best_score_two))\n",
        "print(\"Best parameters: \" + str(best_params_two))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrF6cx7IDXzZ"
      },
      "source": [
        "> Next, let's see what the best model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCS-ZVosDXzb"
      },
      "source": [
        "# Initialize the best model\n",
        "best_params_two = {'width': plane.shape[0], 'height': plane.shape[1], 'depth': plane.shape[2],\n",
        "                   'learning_rate': 0.001, 'activation': Softmax, 'optimizer': Adagrad}\n",
        "model_two = create_cnn_model_two(**best_params_two)\n",
        "model_two.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC9kdJkiDXzk"
      },
      "source": [
        "> Finally, let's validate this best accuracy using the method defined before with data augmentation and 10-cross fold validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WyD38Q8DXzn"
      },
      "source": [
        "results_two = validate_model(indicies,\n",
        "                              converted_dataset, outputs_onehot, outputs,\n",
        "                              best_params_two,\n",
        "                              create_cnn_model = create_cnn_model_two)\n",
        "print(results_two)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSByWFY-DcEt"
      },
      "source": [
        "---\n",
        "**Model 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osvqTCnfDcEz"
      },
      "source": [
        "> Next, let's run a grid search on the model to determine the best parameters for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kMuBm5_DcE3"
      },
      "source": [
        "# Define the model\n",
        "classifier = KerasClassifier(build_fn=create_cnn_model_three, verbose=0)\n",
        "\n",
        "# Define the parameters\n",
        "width = [plane.shape[0]]\n",
        "height = [plane.shape[1]]\n",
        "depth = [plane.shape[2]]\n",
        "learning_rates = [0.001]\n",
        "activations = [ReLU, Softmax]\n",
        "optimizers = [Adagrad, Adam]\n",
        "\n",
        "# Turn it into a dictionary for the grid\n",
        "grid = dict(width = width,\n",
        "            height = height,\n",
        "            depth = depth,\n",
        "            learning_rate = learning_rates,\n",
        "            activation = activations,\n",
        "            optimizer = optimizers)\n",
        "\n",
        "# Run the grid search\n",
        "best_score_three, best_params_three, _ = model_grid_search(classifier, grid, \n",
        "                                             converted_dataset, outputs_onehot)\n",
        "print(\"Best score: \" + str(best_score_three))\n",
        "print(\"Best parameters: \" + str(best_params_three))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIbGoPA1DcFB"
      },
      "source": [
        "> Next, let's see what the best model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHGTvXSFDcFF"
      },
      "source": [
        "# Initialize the best model\n",
        "best_params_three = {'width': plane.shape[0], 'height': plane.shape[1], 'depth': plane.shape[2],\n",
        "                   'learning_rate': 0.001, 'activation': ReLU, 'optimizer': Adam}\n",
        "model_three = create_cnn_model_three(**best_params_three)\n",
        "model_three.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djjy1WWqDcFM"
      },
      "source": [
        "> Finally, let's validate this best accuracy using the method defined before with data augmentation and 10-cross fold validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_pvIQbuDcFQ"
      },
      "source": [
        "results_three = validate_model(indicies,\n",
        "                               converted_dataset, outputs_onehot, outputs,\n",
        "                               best_params_three,\n",
        "                               create_cnn_model = create_cnn_model_three)\n",
        "print(results_three)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoOmuSRiI39e"
      },
      "source": [
        "\n",
        "# Model Evaluation: Ships"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hbvpEk6I39z"
      },
      "source": [
        "\n",
        "\n",
        "> First, let's prepare the dataset by converting each of the images to the author's suggestion as shown before\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLb6nhzpI391"
      },
      "source": [
        "# Initialize a list for the converted \n",
        "converted_dataset = list()\n",
        "\n",
        "# Cycle through each instance in the dataset\n",
        "for ship in ships_dataset['data']:\n",
        "    ship = np.array(ship).astype('uint8')\n",
        "    ship = ship.reshape((3, 6400)).T.reshape((80, 80, 3))\n",
        "    converted_dataset.append(ship)\n",
        "\n",
        "# Convert the final list into a numpy array\n",
        "converted_dataset = np.asarray(converted_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PdPwmnDKLf2"
      },
      "source": [
        "> Next, let's get the outputs and split our dataset using the StratifiedKFold object defined before, and convert the outputs to onehot encoding using Keras' to_categorical method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgVBZwbLI39-"
      },
      "source": [
        "# Split the dataset into 10 folds and store the indicies\n",
        "outputs = np.array(ships_dataset['labels']).astype('uint8')\n",
        "indicies = list(kf.split(converted_dataset, outputs))\n",
        "\n",
        "# Convert the outputs to onehot encoding\n",
        "from keras.utils import to_categorical\n",
        "outputs_onehot = to_categorical(outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCA2F8yyDzQF"
      },
      "source": [
        "---\n",
        "**Model 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD_9rqzt-B6H"
      },
      "source": [
        "> Next, let's run a grid search on the model to determine the best parameters for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ_cWYfx-B6S"
      },
      "source": [
        "# Define the model\n",
        "classifier = KerasClassifier(build_fn=create_cnn_model_one, verbose=0)\n",
        "\n",
        "# Define the parameters\n",
        "width = [ship.shape[0]]\n",
        "height = [ship.shape[1]]\n",
        "depth = [ship.shape[2]]\n",
        "learning_rates = [0.001]\n",
        "activations = [ReLU, Softmax]\n",
        "optimizers = [Adagrad, Adam]\n",
        "\n",
        "# Turn it into a dictionary for the grid\n",
        "grid = dict(width = width,\n",
        "            height = height,\n",
        "            depth = depth,\n",
        "            learning_rate = learning_rates,\n",
        "            activation = activations,\n",
        "            optimizer = optimizers)\n",
        "\n",
        "# Run the grid search\n",
        "best_score_one, best_params_one, _ = model_grid_search(classifier, grid, \n",
        "                                             converted_dataset, outputs_onehot)\n",
        "print(\"Best score: \" + str(best_score_one))\n",
        "print(\"Best parameters: \" + str(best_params_one))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is6XRYTmG5Wr"
      },
      "source": [
        "> Next, let's see what the best model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrVxdq8DG5Wr"
      },
      "source": [
        "# Initialize the best model\n",
        "best_params_one = {'width': ship.shape[0], 'height': ship.shape[1], 'depth': ship.shape[2],\n",
        "                   'learning_rate': 0.001, 'activation': Softmax, 'optimizer': Adam}\n",
        "model_one = create_cnn_model_one(**best_params_one)\n",
        "model_one.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLt8W4_a-B6m"
      },
      "source": [
        "> Finally, let's validate the accuracy using method defined before with data augmentation and 10-cross fold validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOY-nj5M-B6r"
      },
      "source": [
        "results_one = validate_model(indicies,\n",
        "                             converted_dataset, outputs_onehot, outputs,\n",
        "                             best_params_one,\n",
        "                             create_cnn_model = create_cnn_model_one)\n",
        "print(results_one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewRU-LzYD44d"
      },
      "source": [
        "---\n",
        "**Model 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZNxYWIGD44k"
      },
      "source": [
        "> Next, let's run a grid search on the model to determine the best parameters for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdXGAaCaD44o"
      },
      "source": [
        "# Define the model\n",
        "classifier = KerasClassifier(build_fn=create_cnn_model_two, verbose=0)\n",
        "\n",
        "# Define the parameters\n",
        "width = [ship.shape[0]]\n",
        "height = [ship.shape[1]]\n",
        "depth = [ship.shape[2]]\n",
        "learning_rates = [0.001]\n",
        "activations = [ReLU, Softmax]\n",
        "optimizers = [Adagrad, Adam]\n",
        "\n",
        "# Turn it into a dictionary for the grid\n",
        "grid = dict(width = width,\n",
        "            height = height,\n",
        "            depth = depth,\n",
        "            learning_rate = learning_rates,\n",
        "            activation = activations,\n",
        "            optimizer = optimizers)\n",
        "\n",
        "# Run the grid search\n",
        "best_score_two, best_params_two, _ = model_grid_search(classifier, grid, \n",
        "                                             converted_dataset, outputs_onehot)\n",
        "print(\"Best score: \" + str(best_score_two))\n",
        "print(\"Best parameters: \" + str(best_params_two))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96U-p9eoD44z"
      },
      "source": [
        "> Next, let's see what the best model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tt1SkriD443"
      },
      "source": [
        "# Initialize the best model\n",
        "best_params_two = {'width': ship.shape[0], 'height': ship.shape[1], 'depth': ship.shape[2],\n",
        "                   'learning_rate': 0.001, 'activation': Softmax, 'optimizer': Adam}\n",
        "model_two = create_cnn_model_two(**best_params_two)\n",
        "model_two.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFFySMJ-D44-"
      },
      "source": [
        "> Finally, let's validate the accuracy using method defined before with data augmentation and 10-cross fold validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePy8qtqFD45A"
      },
      "source": [
        "results_two = validate_model(indicies,\n",
        "                               converted_dataset, outputs_onehot, outputs,\n",
        "                               best_params_two,\n",
        "                               create_cnn_model = create_cnn_model_two)\n",
        "print(results_two)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIkz0vDID7Y1"
      },
      "source": [
        "---\n",
        "**Model 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ2Q9IWiD7Y6"
      },
      "source": [
        "> Next, let's run a grid search on the model to determine the best parameters for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYWtxL4gD7Y9"
      },
      "source": [
        "# Define the model\n",
        "classifier = KerasClassifier(build_fn=create_cnn_model_three, verbose=0)\n",
        "\n",
        "# Define the parameters\n",
        "width = [ship.shape[0]]\n",
        "height = [ship.shape[1]]\n",
        "depth = [ship.shape[2]]\n",
        "learning_rates = [0.001]\n",
        "activations = [ReLU, Softmax]\n",
        "optimizers = [Adagrad, Adam]\n",
        "\n",
        "# Turn it into a dictionary for the grid\n",
        "grid = dict(width = width,\n",
        "            height = height,\n",
        "            depth = depth,\n",
        "            learning_rate = learning_rates,\n",
        "            activation = activations,\n",
        "            optimizer = optimizers)\n",
        "\n",
        "# Run the grid search\n",
        "best_score_three, best_params_three, _ = model_grid_search(classifier, grid, \n",
        "                                             converted_dataset, outputs_onehot)\n",
        "print(\"Best score: \" + str(best_score_three))\n",
        "print(\"Best parameters: \" + str(best_params_three))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yt0r9mJD7ZG"
      },
      "source": [
        "> Next, let's see what the best model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Azexm1OD7ZM"
      },
      "source": [
        "# Initialize the best model\n",
        "best_params_three = {'width': ship.shape[0], 'height': ship.shape[1], 'depth': ship.shape[2],\n",
        "                     'learning_rate': 0.001, 'activation': Softmax, 'optimizer': Adam}\n",
        "model_three = create_cnn_model_three(**best_params_three)\n",
        "model_three.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PiSb5ISD7ZT"
      },
      "source": [
        "> Finally, let's validate the accuracy using method defined before with data augmentation and 10-cross fold validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylce1grKD7ZV"
      },
      "source": [
        "results_three = validate_model(indicies,\n",
        "                               converted_dataset, outputs_onehot, outputs,\n",
        "                               best_params_three,\n",
        "                               create_cnn_model = create_cnn_model_three)\n",
        "print(results_three)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P742qU927HQ"
      },
      "source": [
        "# xView Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6plczWVTPeGe"
      },
      "source": [
        "> To train the algorithm on pictures of tents, I will be using a small subset of the [xView dataset](https://arxiv.org/abs/1802.07856). The total size of the data [available to download](https://challenge.xviewdataset.org/data-download) is ~18.2GB so I won't have the option to download it in this notebook but I will show my process to extract the tent/huts images from it then provide a download at the end of this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dg8YBGeG5Wz"
      },
      "source": [
        "> First, let's read the GEOJSON file that contains all of the features in this dataset and see what the first feature looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNyAnhmfG5W0"
      },
      "source": [
        "with open('Datasets/xView/train_labels/xView_train.geojson') as f:\n",
        "    xview_labels = json.load(f)\n",
        "\n",
        "print(\"Here's what the first feature in the dataset looks like:\\n\")\n",
        "print(xview_labels['features'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jK1FNCQG5W3"
      },
      "source": [
        "> Next, let's see what this first feature is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkDWeBr6G5W5"
      },
      "source": [
        "print(\"Here's the type of feature for this first entry:\\n\")\n",
        "print(xview_labels['features'][0]['properties']['type_id'])\n",
        "print(\"\\nThis type is referred to as \\\"Building\\\"\")\n",
        "print(\"https://github.com/DIUx-xView/xview2018-baseline/blob/master/xview_class_labels.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s-_kauOG5W8"
      },
      "source": [
        "> Next, let's see what the filename is that this feature is in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9x3i9xBG5W8"
      },
      "source": [
        "print(\"Here's the filename of the first entry:\\n\")\n",
        "print(xview_labels['features'][0]['properties']['image_id'])\n",
        "print(\"\\nNote that the images available from the website don't represent the entire dataset so many are missing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F8bD-GGG5XA"
      },
      "source": [
        "> Next, let's see how many images in this dataset contain the feature we're after: \"Hut/Tent\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yntPBh46G5XB"
      },
      "source": [
        "# 71 == hut/tent\n",
        "tent_files = []\n",
        "tent_features = []\n",
        "\n",
        "for feature in xview_labels['features']:\n",
        "    if feature['properties']['type_id'] == 71:\n",
        "        if feature['properties']['image_id'] not in tent_files:\n",
        "            tent_files.append(feature['properties']['image_id'])\n",
        "        tent_features.append(feature)\n",
        "        \n",
        "num_files = len(tent_files)\n",
        "print(\"The total number of files with features classified as type \\\"Hut/Tent\\\" is \" + str(num_files))\n",
        "print(\"The total number of features (i.e. actual tents/huts) in the \" + str(num_files) + \" files is \" + str(len(tent_features)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwhdXK0QG5XD"
      },
      "source": [
        "> Next, out of these files, let's see how many are actually given to us"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe5NVksZG5XE"
      },
      "source": [
        "file_paths = {}\n",
        "found_files = []\n",
        "for file in tent_files:\n",
        "    res = glob.glob('Datasets\\\\xView\\\\**/' + file, recursive=True)\n",
        "    if len(res) > 0:\n",
        "        file_paths[file] = res[0]\n",
        "        found_files.append(file)\n",
        "\n",
        "num_actual_files = len(found_files)\n",
        "print(\"The number of files with tents/huts that actually exist in this download is \" + str(num_actual_files) + \"/\" + str(num_files))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v8bURIiG5XG"
      },
      "source": [
        "> Then, within these files that are given to us, how many \"Hut/Tent\" features are there in total"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEPC8P21G5XH"
      },
      "source": [
        "found_features = []\n",
        "\n",
        "for feature in tent_features:\n",
        "    if feature['properties']['image_id'] in found_files:\n",
        "        found_features.append(feature)\n",
        "        \n",
        "print(\"The number of features in the \" + str(num_actual_files) + \" files is \" + str(len(found_features)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR_5F___G5XJ"
      },
      "source": [
        "> With these files, let's figure out what size of image we should create. We'll first look at the largest width/height along with the average"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CearKbrG5XJ"
      },
      "source": [
        "# First, let's figure out what the largest height and width is\n",
        "king_width = -1\n",
        "king_height = -1\n",
        "\n",
        "avg_width = 0\n",
        "avg_height = 0\n",
        "for feature in found_features:\n",
        "    bound = [int(x.strip()) for x in feature['properties']['bounds_imcoords'].split(',')]\n",
        "    \n",
        "    height = bound[3] - bound[1]\n",
        "    avg_height += height\n",
        "    if height > king_height:\n",
        "        king_height = height\n",
        "    \n",
        "    width = bound[2] - bound[0]\n",
        "    avg_width += width\n",
        "    if width > king_width:\n",
        "        king_width = width\n",
        "\n",
        "avg_height /= len(found_features)\n",
        "avg_width /= len(found_features)\n",
        "print(\"Largest width  = \" + str(king_width))\n",
        "print(\"Largest height = \" + str(king_height))\n",
        "print()\n",
        "print(\"Average width  = \" + str(avg_width))\n",
        "print(\"Average height = \" + str(avg_height))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPSIsvlzG5XM"
      },
      "source": [
        "> We can see that the largest dimensions are an anomaly and wouldn't give us a good image to train with for the smaller features. The GAN we'll be using needs the image resolution to be a power of two so let's set the image size to 64 x 64 and discard any images with dimensions larger than 64 - 5 = 59 (the boundaries are tight to the objects so we'll give them a small buffer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smMhHQEXG5XM"
      },
      "source": [
        "count = 0\n",
        "for feature in found_features:\n",
        "    bound = [int(x.strip()) for x in feature['properties']['bounds_imcoords'].split(',')]\n",
        "    \n",
        "    height = bound[3] - bound[1]\n",
        "    width = bound[2] - bound[0]\n",
        "    \n",
        "    if height <= 59 and width <= 59:\n",
        "        # Change the bounds so it's exactly 64 x 64\n",
        "        height_diff = 64 - height\n",
        "        width_diff = 64 - width\n",
        "        \n",
        "        height_mod = int(height_diff / 2)\n",
        "        width_mod = int(width_diff / 2)\n",
        "\n",
        "        #Change the bounds to match 64 x 64\n",
        "        tmp = height_mod * 2\n",
        "        if tmp == height_diff:\n",
        "            bound[3] += height_mod\n",
        "            bound[1] -= height_mod\n",
        "        elif tmp > height_diff:\n",
        "            bound[3] += height_mod + 1\n",
        "            bound[1] -= height_mod - (tmp - height_diff) + 1\n",
        "        elif tmp < height_diff:\n",
        "            bound[3] += height_mod + 1\n",
        "            bound[1] -= height_mod - (height_diff - tmp) + 1\n",
        "        \n",
        "        tmp = width_mod * 2\n",
        "        if tmp == width_diff:\n",
        "            bound[2] += width_mod\n",
        "            bound[0] -= width_mod\n",
        "        elif tmp > width_diff:\n",
        "            bound[2] += width_mod + 1\n",
        "            bound[0] -= width_mod - (tmp - width_diff) + 1\n",
        "        elif tmp < width_diff:\n",
        "            bound[2] += width_mod + 1\n",
        "            bound[0] -= width_mod - (width_diff - tmp) + 1\n",
        "        \n",
        "        # Load the image\n",
        "        imageObject  = Image.open(file_paths[feature['properties']['image_id']])\n",
        "        \n",
        "        # Crop and save it\n",
        "        cropped = imageObject.crop((bound[0], bound[1], bound[2], bound[3]))\n",
        "        cropped.save(\"Datasets/xview_tents/\" + str(count) + \".jpg\")\n",
        "\n",
        "        count += 1\n",
        "        \n",
        "print(\"Total images: \" + str(count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEMLzHiFG5XP"
      },
      "source": [
        "> We kept ~89% of the \"Hut/Tent\" features that were available to us with the 64 x 64 constraint. This output is available on my Google Drive so we will download it to this notebook\n",
        "*   https://drive.google.com/file/d/1BLWfARXpf4D1n21JbAt6k2VtyEa6M2t4/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nx_pIBdG5XQ"
      },
      "source": [
        "!gdown --id 1BLWfARXpf4D1n21JbAt6k2VtyEa6M2t4 --output tents.zip\n",
        "!unzip -q tents.zip -d tents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhPfzMEOG5XS"
      },
      "source": [
        "# GAN Creation: StyleGAN2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzrb3ZRYG5XS"
      },
      "source": [
        "> For the generative adversarial network aspect of this project, I will be using an implementation from NVIDIA Labs called 'StyleGAN2'. This takes some time and I end up generating 1,000 images so I will show my process and provide a download link at the end of this section.\n",
        "*   https://github.com/NVlabs/stylegan2\n",
        "*   http://arxiv.org/abs/1912.04958\n",
        "\n",
        "> Let's clone StyleGAN2's repository and verify the install of NVCC in this notebook (code from the repository's README)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im1YC0sKQecF"
      },
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2.git\n",
        "    \n",
        "if colab:\n",
        "    !nvcc /content/stylegan2/test_nvcc.cu -o /content/stylegan2/test_nvcc -run\n",
        "else:\n",
        "    !nvcc stylegan2/test_nvcc.cu -o stylegan2/test_nvcc -run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcqJY4hwTw0d"
      },
      "source": [
        "> Next, let's use their 'dataset_tool.py' file to make a TFRecord dataset of the tents dataset for their GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylx8csNuG5XV"
      },
      "source": [
        "if colab:\n",
        "    !python /content/stylegan2/dataset_tool.py create_from_images /content/tents/tf/tents/ /content/tents/xview_tents/\n",
        "else:\n",
        "    !python stylegan2/dataset_tool.py create_from_images tents/tf/tents/ tents/xview_tents/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BzGURy6G5XY"
      },
      "source": [
        "> Now let's train the GAN on this dataset using the original StyleGAN\n",
        "* Note: you need to use GPU processing enabled for this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe8dk8YcG5XZ"
      },
      "source": [
        "if colab:\n",
        "    !python /content/stylegan2/run_training.py --data-dir=/content/tents/tf/ --dataset=tents --config=config-a --mirror-augment=true --result-dir=/content/tents/result-a/\n",
        "else:\n",
        "    !python stylegan2/run_training.py --data-dir=tents/tf/ --dataset=tents --config=config-a --mirror-augment=true --result-dir=tents/result-a/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQGEYbQK-6TI"
      },
      "source": [
        "> Next, let's generate 1,000 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvRCuPrWJnJw"
      },
      "source": [
        "if colab:\n",
        "  !python /content/stylegan2/run_generator.py generate-images --network=/content/tents/result-a/00000-stylegan2-tents-1gpu-config-a/network-snapshot-000001.pkl --seeds=2020-3019 --truncation-psi=1.0\n",
        "else:\n",
        "  !python stylegan2/run_generator.py generate-images --network=tents/result-a/00000-stylegan2-tents-1gpu-config-a/network-snapshot-000001.pkl --seeds=2020-3019 --truncation-psi=1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-L5dcTOzNw5"
      },
      "source": [
        "> Next, let's train the GAN on the dataset using StyleGAN2\n",
        "* Note: you need to use GPU processing enabled for this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S8cDVh5zNw5"
      },
      "source": [
        "if colab:\n",
        "    !python /content/stylegan2/run_training.py --data-dir=/content/tents/tf/ --dataset=tents --config=config-f --mirror-augment=true --result-dir=/content/tents/result-f/\n",
        "else:\n",
        "    !python stylegan2/run_training.py --data-dir=tents/tf/ --dataset=tents --config=config-f --mirror-augment=true --result-dir=tents/result-f/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sskDTC21zNw7"
      },
      "source": [
        "> Next, let's generate 1,000 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUOvjeI-zNw8"
      },
      "source": [
        "if colab:\n",
        "  !python /content/stylegan2/run_generator.py generate-images --network=/content/tents/result-f/00000-stylegan2-tents-1gpu-config-f/network-snapshot-000000.pkl --seeds=2020-3019 --truncation-psi=1.0\n",
        "else:\n",
        "  !python stylegan2/run_generator.py generate-images --network=tents/result-f/00000-stylegan2-tents-1gpu-config-f/network-snapshot-000000.pkl --seeds=2020-3019 --truncation-psi=1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ2TfwsrB5to"
      },
      "source": [
        "> These outputs are available on my Google Drive so we will download them to this notebook\n",
        "*   https://drive.google.com/file/d/1mak_n_OfgvleUEtu8kp_63ysSsbTVsVT/view?usp=sharing\n",
        "*   https://drive.google.com/file/d/1Q2_LG5Ea8B45HntTdqxLcOQHoEvpiExF/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MjVQQ2NB5tt"
      },
      "source": [
        "!gdown --id 1mak_n_OfgvleUEtu8kp_63ysSsbTVsVT --output output_a.zip\n",
        "!unzip -q output_a.zip -d output_a\n",
        "\n",
        "!gdown --id 1Q2_LG5Ea8B45HntTdqxLcOQHoEvpiExF --output output_f.zip\n",
        "!unzip -q output_f.zip -d output_f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03UX-lKHCsVo"
      },
      "source": [
        "> Let's view an image from the original StyleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FGFnDQpCwed"
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "if colab:\n",
        "  img = mpimg.imread('/content/output_a/seed2500.png')\n",
        "else:\n",
        "  img = mpimg.imread('output_a/seed2500.png')\n",
        "  \n",
        "imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At-5JgwWzNxC"
      },
      "source": [
        "> Finally, let's view an image from the StyleGAN2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rWu4vszNxC"
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "if colab:\n",
        "  img = mpimg.imread('/content/output_f/seed2020.png')\n",
        "else:\n",
        "  img = mpimg.imread('output_f/seed2020.png')\n",
        "  \n",
        "imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU4ZgZGnDFIn"
      },
      "source": [
        "\n",
        "\n",
        "> We can see that the output of the StyleGAN2 configuration is much cleaner/smoother than the StyleGAN configuration so we will use it for testing. Regardless, the output is very abstract in both cases. I'm assuming this has to do with the low resolution of the images I fed to it, as well as the small size of the dataset it learned from. Regardless, we will see how the model performs when training on these images and testing on the actual images. To assist further, all images will be converted to grayscale so the colors don't have a great impact when the model is training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls43Rw1g9R85"
      },
      "source": [
        "# GAN Creation: DCGAN from TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDyEJByj9pZa"
      },
      "source": [
        "> To provide a comparison, I will use the Deep Convolutional Generative Adversarial Network (DCGAN) implementation from TensorFlow:\n",
        "*   https://www.tensorflow.org/tutorials/generative/dcgan\n",
        "\n",
        "> The code from this resource will be pasted below but instead of using the *exact* implementation for the model portion, I will make one based off of it with parameters that can be changed for the purpose of running a grid search to see how it affects the output!\n",
        "*   We will use the summation of the generator and discriminator loss as the measure of performance for this\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kug4hua7J6YX"
      },
      "source": [
        "> First, let's grab the imports they use\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghgd93dOJ3yo"
      },
      "source": [
        "from tensorflow.keras import layers, losses, optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG0bJkMQKexf"
      },
      "source": [
        "> Next, let's adapt their generator model method so we can run a grid search on it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdAm0KtcK0ru"
      },
      "source": [
        "def make_generator_model(width, height, depth,\n",
        "                         activation = layers.LeakyReLU, use_bias = False,\n",
        "                         optimizer = Adam, learning_rate = 0.001):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(int(width / 4)*int(height / 4)*256, use_bias=use_bias, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(activation())\n",
        "\n",
        "    model.add(layers.Reshape((int(width / 4), int(height / 4), 256)))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=use_bias))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(activation())\n",
        "    \n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=use_bias))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(activation())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(depth, (5, 5), strides=(2, 2), padding='same', use_bias=use_bias, activation='tanh'))\n",
        "\n",
        "    optim = optimizer(learning_rate)\n",
        "    return model, optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-CVfKc9K46f"
      },
      "source": [
        "> Next, let's adapt their discriminator model method so we can run a grid search on it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl_MUfguLBr3"
      },
      "source": [
        "def make_discriminator_model(width, height, depth,\n",
        "                             activation = layers.LeakyReLU, dropout  = 0.3,\n",
        "                             optimizer = Adam, learning_rate = 0.001):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[width, height, depth]))\n",
        "    model.add(activation())\n",
        "    model.add(layers.Dropout(dropout))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(activation())\n",
        "    model.add(layers.Dropout(dropout))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    optim = optimizer(learning_rate)\n",
        "    return model, optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E51yW26OLZNf"
      },
      "source": [
        "> Next, let's use their loss method for the discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NsIYGgHLexG"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output, cross_entropy):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-0cOy0CLgpe"
      },
      "source": [
        "> Next, let's use their loss method for the generator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x2-J8WtLmxO"
      },
      "source": [
        "def generator_loss(fake_output, cross_entropy):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI_nR4NKL78Y"
      },
      "source": [
        "\n",
        "\n",
        "> Next, let's use their step method for training, modified to allow for a grid search\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EUHAreeL_vG"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, loss,\n",
        "               generator, discriminator,\n",
        "               generator_optimizer, discriminator_optimizer):\n",
        "\n",
        "    noise = tf.random.normal([len(images), 100])\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output, loss)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output, loss)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    \n",
        "    return gen_loss, disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi8VXiHRMBTf"
      },
      "source": [
        "\n",
        "\n",
        "> Next, let's use adapt method for training the model for our grid search\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOUkshHsMGp2"
      },
      "source": [
        "import tensorflow.contrib.eager as tfe\n",
        "def train(dataset, epochs, loss,\n",
        "          generator, discriminator,\n",
        "          generator_optimizer, discriminator_optimizer):\n",
        "    for epoch in range(epochs):\n",
        "        # Need this for minor difference in TensorFlow 1.X between my machines\n",
        "        try:\n",
        "            for image_batch in tfe.Iterator(dataset):\n",
        "                gen_loss, disc_loss = train_step(image_batch, loss,\n",
        "                                                 generator, discriminator,\n",
        "                                                 generator_optimizer, discriminator_optimizer)\n",
        "        except:\n",
        "            for image_batch in dataset.__iter__():\n",
        "                gen_loss, disc_loss = train_step(image_batch, loss,\n",
        "                                                 generator, discriminator,\n",
        "                                                 generator_optimizer, discriminator_optimizer)\n",
        "            \n",
        "    # Return the end result, total loss\n",
        "    return (gen_loss + disc_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYi5BSxNzNxV"
      },
      "source": [
        "\n",
        "\n",
        "> Next, let's define a custom grid search method for the GAN that's similar to GridSearchCV\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfk1BG2yzNxX"
      },
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "def gan_grid_search(generator_method, discriminator_method,\n",
        "                    generator_grid, discriminator_grid, \n",
        "                    dataset, epochs = 10):\n",
        "    # Convert grids to parameter grids for the generator and discriminator\n",
        "    generator_grid = list(ParameterGrid(generator_grid))\n",
        "    discriminator_grid = list(ParameterGrid(discriminator_grid))\n",
        "    \n",
        "    # Output how many models we'll be testing\n",
        "    total = len(generator_grid) * len(discriminator_grid)\n",
        "    print(\"Fitting \"+ str(total) + \" candidates\")\n",
        "    \n",
        "    # Begin testing the models\n",
        "    count = 0\n",
        "    best_params = {}\n",
        "    best_loss = 1000000\n",
        "    for generator_params in generator_grid:\n",
        "        start = time.time()\n",
        "        for discriminator_params in discriminator_grid:           \n",
        "            # Make the models\n",
        "            generator, generator_optimizer = make_generator_model(**generator_params)\n",
        "            discriminator, discriminator_optimizer = make_discriminator_model(**discriminator_params)\n",
        "            \n",
        "            # Make the loss measure\n",
        "            cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "            \n",
        "            # Train it and get the final, total loss\n",
        "            loss = train(dataset, epochs, cross_entropy,\n",
        "                         generator, discriminator,\n",
        "                         generator_optimizer, discriminator_optimizer)\n",
        "            \n",
        "            # Store parameters if it is the best\n",
        "            if(loss < best_loss):\n",
        "                best_loss = loss\n",
        "                best_params['generator'] = generator_params\n",
        "                best_params['discriminator'] = discriminator_params\n",
        "            count += 1\n",
        "            \n",
        "        # Output progress\n",
        "        print(\"Done\\t\" + str(count) + \" out of \" + str(total) + \" | elapsed: \" + str(((time.time()-start) / 60)) + \"min\")\n",
        "        \n",
        "    return best_params, best_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPTQVv9uzNxY"
      },
      "source": [
        "> Next, let's create the dataset of real images of tents and split them into batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIxc1m1-zNxY"
      },
      "source": [
        "# Get all of the real images of tents into numpy arrays\n",
        "path_preceed = '/content/' if colab else ''\n",
        "files = glob.glob(path_preceed + \"Datasets/xview_tents/xview_tents/*.jpg\")\n",
        "dataset = []\n",
        "for file in files:\n",
        "    # Convert to numpy array\n",
        "    img = PIL.Image.open(file)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = np.asarray(img, dtype=np.float32) / 255\n",
        "    img = img[:, :, :3]\n",
        "    dataset.append(img)\n",
        "    \n",
        "# Split them into evenly sized batches\n",
        "batch_size = 2\n",
        "while(len(dataset) % batch_size != 0):\n",
        "    batch_size += 1\n",
        "    \n",
        "dataset_split = []\n",
        "for i in range(batch_size, len(dataset) + 1, batch_size):\n",
        "    dataset_split.append(dataset[i - batch_size : i])\n",
        "splits = len(dataset_split)\n",
        "dataset = np.asarray(dataset_split)\n",
        "    \n",
        "# Using code from the DCGNN example\n",
        "dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(2020)\n",
        "print(\"Using a batch size of \" + str(batch_size) + \" for a total of \" + str(splits) + \" splits in the dataset!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXP2CmQ1zNxa"
      },
      "source": [
        "> Next, let's define the parameters to run in the grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYvxrEJGzNxa"
      },
      "source": [
        "# Define the grid search parameters\n",
        "width = [img.shape[0]]\n",
        "height = [img.shape[1]]\n",
        "depth = [img.shape[2]]\n",
        "activations = [layers.Softmax, layers.LeakyReLU]\n",
        "optimizers = [tf.keras.optimizers.Adagrad, tf.keras.optimizers.Adam]\n",
        "\n",
        "# Turn them into dictionaries\n",
        "generator_grid = dict(width = width,\n",
        "                    height = height,\n",
        "                    depth = depth,\n",
        "                    activation = activations,\n",
        "                    use_bias = [True, False],\n",
        "                    optimizer = optimizers)\n",
        "discriminator_grid = dict(width = width,\n",
        "                    height = height,\n",
        "                    depth = depth,\n",
        "                    activation = activations,\n",
        "                    dropout = [0.3, 0.4],\n",
        "                    optimizer = optimizers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKD0CFZWzNxc"
      },
      "source": [
        "> Finally, let's run the grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcjkY5-FzNxc"
      },
      "source": [
        "best_params, best_loss = gan_grid_search(make_generator_model, make_discriminator_model,\n",
        "                                         generator_grid, discriminator_grid, \n",
        "                                         dataset, batch_size)\n",
        "print(best_params)\n",
        "print(best_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32qnsGIQzNxd"
      },
      "source": [
        "> Now, with this configuration, let's train it on the real tents for a much longer duration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lttQ9WSzNxd"
      },
      "source": [
        "# Make the models\n",
        "best_params = {'generator': {'activation': layers.LeakyReLU,\n",
        "   'depth': 3,\n",
        "   'height': 64,\n",
        "   'optimizer': tf.keras.optimizers.Adagrad,\n",
        "   'use_bias': True,\n",
        "   'width': 64,\n",
        "    'learning_rate': 1e-3},\n",
        "  'discriminator': {'activation': layers.LeakyReLU,\n",
        "   'depth': 3,\n",
        "   'dropout': 0.4,\n",
        "   'height': 64,\n",
        "   'optimizer': tf.keras.optimizers.Adam,\n",
        "   'width': 64,\n",
        "   'learning_rate': 1e-3}}\n",
        "\n",
        "generator, generator_optimizer = make_generator_model(**best_params['generator'])\n",
        "discriminator, discriminator_optimizer = make_discriminator_model(**best_params['discriminator'])\n",
        "\n",
        "# Make the loss measure\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# Train it and get the final, total loss\n",
        "loss = train(dataset, 75, cross_entropy,\n",
        "             generator, discriminator,\n",
        "             generator_optimizer, discriminator_optimizer)\n",
        "print(\"Training complete with a loss of \" + str(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH760KKuzNxf"
      },
      "source": [
        "> Next, let's generate a set of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3PEJLNXzNxf"
      },
      "source": [
        "noise = tf.random.normal([1000, 100])\n",
        "color_generated_dataset = generator(noise, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ldc-McO9HEg"
      },
      "source": [
        "> I have this on my Google Drive so let's download the output\n",
        "*   https://drive.google.com/file/d/11nLJsGGVCt6nHiv3FqsnNXNcbvdf-v0Y/view?usp=sharing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhzopmcs2jDO"
      },
      "source": [
        "!gdown --id 11nLJsGGVCt6nHiv3FqsnNXNcbvdf-v0Y --output dcgan_tents.pkl\n",
        "import pickle\n",
        "with open('dcgan_tents.pkl', 'rb') as dcgan_results:\n",
        "    color_generated_dataset = pickle.load(dcgan_results)\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default(): \n",
        "      color_generated_dataset = color_generated_dataset.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C-1a3A12jO1"
      },
      "source": [
        "> Finally, let's view an image from this generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5DO0kKd9EAn"
      },
      "source": [
        "plt.imshow(color_generated_dataset[22][:, :, :3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM_SJ1YozlGO"
      },
      "source": [
        "> We can see that the generation is so much better in quality. I assume this is because of the model having a much lower complexity than that of the StyleGAN2 which is good for our low quanitity + resolution dataset. We will see the performance of the three models on this later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlLShcT9MDoz"
      },
      "source": [
        "# Save the images to files\n",
        "def normalize(x):\n",
        "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))\n",
        "\n",
        "for i in range(len(color_generated_dataset)):\n",
        "    img = normalize(color_generated_dataset[i][:, :, :3])\n",
        "    plt.imsave('Datasets/DCGAN/dcgan_tent' + str(i) + '.png', img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE7lS2FYILP1"
      },
      "source": [
        "# Model Evaluation: Tents via StyleGAN2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKSL36pTIVNI"
      },
      "source": [
        "> To properly evaluate the model, I am going to download a dataset I've made on my Google Drive using the past two sections, as well as adding in a variety of different types of features from the xView dataset to make our training/testing dataset complete. The two classes here will be \"Tent\" or \"No Tent\".\n",
        "*    Specifically, I used features classified as \"Shed\" and \"Building\". Without any constraints, the xView dataset had 150,329 of these available to me. The average size was below the constraint of 64 x 64 so I just randomly grabbed 1,000 to add to training, and 200 to add to testing\n",
        "*    In total, we have 2,000 images for training and 418 for testing (roughly an 80/20 split and even distribution of the classes)\n",
        "*    https://drive.google.com/file/d/1otKjaXNQIOoIkfGpfabNJaorBIkLXAeT/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh_iNBvUIP4s"
      },
      "source": [
        "!gdown --id 1otKjaXNQIOoIkfGpfabNJaorBIkLXAeT --output tentsnet.zip\n",
        "!unzip -q tentsnet.zip -d tentsnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzX_zGdqNrf4"
      },
      "source": [
        "> Since the dataset is split into distinct **test** and **train** sets (real and synthetic), we can't do ten cross-fold-validation as we did before. Instead, we will take the accuracy at the end of training for 50 epochs on the dataset in the following method adapted from my validation method before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUfd3nahN9o-"
      },
      "source": [
        "def validate_tent_model(X_train, X_test, Y_train, Y_test,\n",
        "                        create_cnn_model, params, epochs,\n",
        "                        outputs_indiv):\n",
        "    start = time.time()\n",
        "\n",
        "    # Initialize a model\n",
        "    if params == None:\n",
        "        model = create_cnn_model(len(inputs[0]),\n",
        "                              len(inputs[0][0]),\n",
        "                              len(inputs[0][0][0]))\n",
        "    else:\n",
        "        model = create_cnn_model(**params)\n",
        "\n",
        "    # Compute the class weights\n",
        "    weights = class_weight.compute_class_weight('balanced',\n",
        "                                                np.unique(outputs_indiv), \n",
        "                                                outputs_indiv)\n",
        "\n",
        "    # Train and test the model on this dataset\n",
        "    result = model.fit(X_train, Y_train,\n",
        "                        batch_size = 8,\n",
        "                        epochs = epochs,\n",
        "                        validation_data = (X_test, Y_test),\n",
        "                        class_weight = weights,\n",
        "                        verbose = 0)\n",
        "\n",
        "    # Print the results and return the model with them\n",
        "    end = time.time()\n",
        "    print(\"Completed in \" + str(end - start) + \"s\" + \n",
        "        \"\\n\\tTrain accuracy\\t= \" + str(result.history['acc'][len(result.history['acc']) - 1]) + \n",
        "        \"\\n\\tTrain loss\\t= \" + str(result.history['loss'][len(result.history['loss']) - 1]) +\n",
        "        \"\\n\\tTest accuracy\\t= \" + str(result.history['val_acc'][len(result.history['val_acc']) - 1]) +\n",
        "        \"\\n\\tTest loss\\t= \" + str(result.history['val_loss'][len(result.history['val_loss']) - 1]))\n",
        "    \n",
        "    return model, result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vQfD90bSiNi"
      },
      "source": [
        "> Next, lets read each image as a 3D numpy array and store their class\n",
        "*   I followed the naming convention \"class_dataset #.jpg\" where class = 0 for no tent or 1 for tent and dataset = test or train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YOMgDL1Sg9z"
      },
      "source": [
        "path_preceed = '/content/' if colab else ''\n",
        "\n",
        "test_files = glob.glob(path_preceed + \"tentsnet/tentsnet/test/*.jpg\") + glob.glob(path_preceed + \"tentsnet/tentsnet/test/*.png\")\n",
        "test_images = []\n",
        "test_classes = []\n",
        "for file in test_files:\n",
        "  if \"0_\" in file:\n",
        "    test_classes.append(0)\n",
        "  else:\n",
        "    test_classes.append(1)\n",
        "  \n",
        "  # Append the image\n",
        "  img = PIL.Image.open(file)\n",
        "  img = np.asarray(img)[:, :, :3]/255\n",
        "  img = np.pad(img, ((6, 6), (6, 6), (0, 0)),\n",
        "                   mode='constant', constant_values=3)\n",
        "  test_images.append(img)\n",
        "\n",
        "train_files = glob.glob(path_preceed + \"tentsnet/tentsnet/train/*.jpg\") + glob.glob(path_preceed + \"tentsnet/tentsnet/train/*.png\")\n",
        "train_images = []\n",
        "train_classes = []\n",
        "for file in train_files:\n",
        "  if \"0_\" in file:\n",
        "    train_classes.append(0)\n",
        "  else:\n",
        "    train_classes.append(1)\n",
        "\n",
        "  # Append the image\n",
        "  img = PIL.Image.open(file)\n",
        "  img = np.asarray(img)[:, :, :3]/255\n",
        "  img = np.pad(img, ((6, 6), (6, 6), (0, 0)),\n",
        "                   mode='constant', constant_values=3)\n",
        "  train_images.append(img)\n",
        "\n",
        "# Convert them all to numpy\n",
        "test_images = np.asarray(test_images)\n",
        "test_classes = np.asarray(test_classes)\n",
        "\n",
        "train_images = np.asarray(train_images)\n",
        "train_classes = np.asarray(train_classes)\n",
        "\n",
        "print(\"Number of training files: \" + str(len(train_images)))\n",
        "print(\"Number of testing files: \" + str(len(test_images)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twwqWDABW1Ip"
      },
      "source": [
        "> Next, let's convert the outputs to onehot encoding using Keras' to_categorical method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFeskpZzW1Iv"
      },
      "source": [
        "# Convert the outputs to onehot encoding\n",
        "from keras.utils import to_categorical\n",
        "test_outputs_onehot = to_categorical(test_classes)\n",
        "train_outputs_onehot = to_categorical(train_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ95KfI13E_x"
      },
      "source": [
        "---\n",
        "**Model 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn2zgWP53E_0"
      },
      "source": [
        "> Next, let's run a grid search on the model to determine the best parameters for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFRnVGLN3E_3"
      },
      "source": [
        "# Define the model\n",
        "classifier = KerasClassifier(build_fn=create_cnn_model_one, verbose=0)\n",
        "\n",
        "# Define the parameters\n",
        "width = [train_images[0].shape[0]]\n",
        "height = [train_images[0].shape[1]]\n",
        "depth = [train_images[0].shape[2]]\n",
        "learning_rates = [0.001]\n",
        "activations = [ReLU, Softmax]\n",
        "optimizers = [Adagrad, Adam]\n",
        "\n",
        "# Turn it into a dictionary for the grid\n",
        "grid = dict(width = width,\n",
        "            height = height,\n",
        "            depth = depth,\n",
        "            learning_rate = learning_rates,\n",
        "            activation = activations,\n",
        "            optimizer = optimizers)\n",
        "\n",
        "# Run the grid search\n",
        "best_score_one, best_params_one, grid_one = model_grid_search(classifier, grid, \n",
        "                                             train_images, train_outputs_onehot)\n",
        "print(\"Best score: \" + str(best_score_one))\n",
        "print(\"Best parameters: \" + str(best_params_one))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_bpftXG3E_-"
      },
      "source": [
        "> Next, let's see what the best model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE4s88Gm3E__"
      },
      "source": [
        "# Initialize the best model\n",
        "best_params_one = {'width': train_images[0].shape[0], 'height': train_images[0].shape[1], 'depth': train_images[0].shape[2],\n",
        "                   'learning_rate': 0.01, 'activation': ReLU, 'optimizer': Adagrad}\n",
        "model_one = create_cnn_model_one(**best_params_one)\n",
        "model_one.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFv0QEGi3FAG"
      },
      "source": [
        "> Finally, let's evaluate the optimized model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0DFqsr_3FAG"
      },
      "source": [
        "validate_tent_model(train_images, test_images, \n",
        "                    train_outputs_onehot, test_outputs_onehot,\n",
        "                    create_cnn_model_one, best_params_one, 10,\n",
        "                    train_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6kSdjwk2smz"
      },
      "source": [
        "---\n",
        "**Model 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT4gXYqg2sm3"
      },
      "source": [
        "> Next, let's run a grid search on the model to determine the best parameters for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmXaJTU42sm5"
      },
      "source": [
        "# Define the model\n",
        "classifier = KerasClassifier(build_fn=create_cnn_model_two, verbose=0)\n",
        "\n",
        "# Define the parameters\n",
        "width = [train_images[0].shape[0]]\n",
        "height = [train_images[0].shape[1]]\n",
        "depth = [train_images[0].shape[2]]\n",
        "learning_rates = [0.001]\n",
        "activations = [ReLU, Softmax]\n",
        "optimizers = [Adagrad, Adam]\n",
        "\n",
        "# Turn it into a dictionary for the grid\n",
        "grid = dict(width = width,\n",
        "            height = height,\n",
        "            depth = depth,\n",
        "            learning_rate = learning_rates,\n",
        "            activation = activations,\n",
        "            optimizer = optimizers)\n",
        "\n",
        "# Run the grid search\n",
        "best_score_two, best_params_two, grid_two = model_grid_search(classifier, grid, \n",
        "                                             train_images, train_outputs_onehot)\n",
        "print(\"Best score: \" + str(best_score_two))\n",
        "print(\"Best parameters: \" + str(best_params_two))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7izKJRwI2snB"
      },
      "source": [
        "> Next, let's see what the best model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxzvPtx22snC"
      },
      "source": [
        "# Initialize the best model\n",
        "best_params_two = {'width': train_images[0].shape[0], 'height': train_images[0].shape[1], 'depth': train_images[0].shape[2],\n",
        "                   'learning_rate': 0.01, 'activation': ReLU, 'optimizer': Adagrad}\n",
        "model_two = create_cnn_model_two(**best_params_two)\n",
        "model_two.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lVIAOVk2snQ"
      },
      "source": [
        "> Finally, let's evaluate the optimized model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRApalfK2snR"
      },
      "source": [
        "validate_tent_model(train_images, test_images, \n",
        "                    train_outputs_onehot, test_outputs_onehot,\n",
        "                    create_cnn_model_two, best_params_two, 10,\n",
        "                    train_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxLevj5wokZg"
      },
      "source": [
        "---\n",
        "**Model 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAAq0R4nokZj"
      },
      "source": [
        "> Next, let's run a grid search on the model to determine the best parameters for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIOglV7qokZj"
      },
      "source": [
        "# Define the model\n",
        "classifier = KerasClassifier(build_fn=create_cnn_model_three, verbose=0)\n",
        "\n",
        "# Define the parameters\n",
        "width = [train_images[0].shape[0]]\n",
        "height = [train_images[0].shape[1]]\n",
        "depth = [train_images[0].shape[2]]\n",
        "learning_rates = [0.001]\n",
        "activations = [ReLU, Softmax]\n",
        "optimizers = [Adagrad, Adam]\n",
        "\n",
        "# Turn it into a dictionary for the grid\n",
        "grid = dict(width = width,\n",
        "            height = height,\n",
        "            depth = depth,\n",
        "            learning_rate = learning_rates,\n",
        "            activation = activations,\n",
        "            optimizer = optimizers)\n",
        "\n",
        "# Run the grid search\n",
        "best_score_three, best_params_three, grid_three = model_grid_search(classifier, grid, \n",
        "                                             train_images, train_outputs_onehot)\n",
        "print(\"Best score: \" + str(best_score_three))\n",
        "print(\"Best parameters: \" + str(best_params_three))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEc6sBJYokZr"
      },
      "source": [
        "> Next, let's see what the best model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5GzdXi4okZt"
      },
      "source": [
        "# Initialize the best model\n",
        "best_params_three = {'width': train_images[0].shape[0], 'height': train_images[0].shape[1], 'depth': train_images[0].shape[2],\n",
        "                   'learning_rate': 0.01, 'activation': Softmax, 'optimizer': Adagrad}\n",
        "model_three = create_cnn_model_three(**best_params_three)\n",
        "model_three.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIyBTJIuYZfh"
      },
      "source": [
        "> Finally, we're evaluating this dataset a bit differently than the planes and ships ones since we have a defined test dataset that can't be mixed with the training dataset. So, we'll use the resulting model from the grid search done previously and have it predict the test dataset then show a classification report from sklearn based on the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chP8xekSYY3K"
      },
      "source": [
        "validate_tent_model(train_images, test_images, \n",
        "                    train_outputs_onehot, test_outputs_onehot,\n",
        "                    create_cnn_model_three, best_params_three, 10,\n",
        "                    train_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dvkeAAN-8Ph"
      },
      "source": [
        "# Model Evaluation: Tents via TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaHt4dXn0CNG"
      },
      "source": [
        "> We are going to use the same download source as before for the \"tentsnet\" dataset and substitute in the generated \"color_generated_dataset\" from the previous methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mK2o_b60CNZ"
      },
      "source": [
        "> Let's read each image as a 3D numpy array and store their class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BIKKaHh0CNb"
      },
      "source": [
        "path_preceed = '/content/' if colab else ''\n",
        "\n",
        "test_files = glob.glob(path_preceed + \"tentsnet/tentsnet/test/*.jpg\") + glob.glob(path_preceed + \"tentsnet/tentsnet/test/*.png\")\n",
        "test_images = []\n",
        "test_classes = []\n",
        "for file in test_files:\n",
        "  if \"0_\" in file:\n",
        "    test_classes.append(0)\n",
        "  else:\n",
        "    test_classes.append(1)\n",
        "  \n",
        "  # Add to the dataset\n",
        "  img = PIL.Image.open(file)\n",
        "  img = np.asarray(img)[:, :, :3]/255\n",
        "  img = np.pad(img, ((6, 6), (6, 6), (0, 0)),\n",
        "                   mode='constant', constant_values=3)\n",
        "  test_images.append(img)\n",
        "\n",
        "train_files = glob.glob(path_preceed + \"tentsnet/tentsnet/train/*.jpg\") + glob.glob(path_preceed + \"tentsnet/tentsnet/train/*.png\")\n",
        "train_images = []\n",
        "train_classes = []\n",
        "\n",
        "counter = 0\n",
        "for file in train_files:\n",
        "  if \"0_\" in file:\n",
        "    train_classes.append(0)\n",
        "\n",
        "    # Add to the dataset\n",
        "    img = PIL.Image.open(file)\n",
        "    img = np.asarray(img)[:, :, :3]/255\n",
        "    img = np.pad(img, ((6, 6), (6, 6), (0, 0)),\n",
        "                    mode='constant', constant_values=3)\n",
        "    train_images.append(img)\n",
        "  else:\n",
        "    # This is a generated tent; use the color images from the DCGAN model\n",
        "    train_classes.append(1)\n",
        "    img = color_generated_dataset[counter][:, :, :3]\n",
        "    img = np.pad(img, ((6, 6), (6, 6), (0, 0)),\n",
        "                    mode='constant', constant_values=3)\n",
        "    train_images.append(img)\n",
        "    counter += 1\n",
        "\n",
        "# Convert them all to numpy\n",
        "test_images = np.asarray(test_images)\n",
        "test_classes = np.asarray(test_classes)\n",
        "\n",
        "train_images = np.asarray(train_images)\n",
        "train_classes = np.asarray(train_classes)\n",
        "\n",
        "print(\"Number of training files: \" + str(len(train_images)))\n",
        "print(\"Number of testing files: \" + str(len(test_images)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM4nIC4S0COB"
      },
      "source": [
        "> Next, let's convert the outputs to onehot encoding using Keras' to_categorical method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33vwLpv60COC"
      },
      "source": [
        "# Convert the outputs to onehot encoding\n",
        "from keras.utils import to_categorical\n",
        "test_outputs_onehot = to_categorical(test_classes)\n",
        "train_outputs_onehot = to_categorical(train_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH1oysoZ3Yfv"
      },
      "source": [
        "---\n",
        "**Model 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78LlI35hCkWP"
      },
      "source": [
        "> Next, let's run a grid search on the model to determine the best parameters for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uyKfE8zCkWS"
      },
      "source": [
        "# Define the model\n",
        "classifier = KerasClassifier(build_fn=create_cnn_model_one, verbose=0)\n",
        "\n",
        "# Define the parameters\n",
        "width = [train_images[0].shape[0]]\n",
        "height = [train_images[0].shape[1]]\n",
        "depth = [train_images[0].shape[2]]\n",
        "learning_rates = [0.001]\n",
        "activations = [ReLU, Softmax]\n",
        "optimizers = [Adagrad, Adam]\n",
        "\n",
        "# Turn it into a dictionary for the grid\n",
        "grid = dict(width = width,\n",
        "            height = height,\n",
        "            depth = depth,\n",
        "            learning_rate = learning_rates,\n",
        "            activation = activations,\n",
        "            optimizer = optimizers)\n",
        "\n",
        "# Run the grid search\n",
        "best_score_one, best_params_one, grid_one = model_grid_search(classifier, grid, \n",
        "                                             train_images, train_outputs_onehot)\n",
        "print(\"Best score: \" + str(best_score_one))\n",
        "print(\"Best parameters: \" + str(best_params_one))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TlrPVD23Yf5"
      },
      "source": [
        "> Next, let's see what the best model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytRgXQ-M3Yf6"
      },
      "source": [
        "# Initialize the best model\n",
        "best_params_one = {'width': train_images[0].shape[0], 'height': train_images[0].shape[1], 'depth': train_images[0].shape[2],\n",
        "                   'learning_rate': 0.01, 'activation': Softmax, 'optimizer': Adagrad}\n",
        "model_one = create_cnn_model_one(**best_params_one)\n",
        "model_one.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OBt-eU73YgA"
      },
      "source": [
        "> Finally, let's evaluate the optimized model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3UFla1X3YgB"
      },
      "source": [
        "validate_tent_model(train_images, test_images, \n",
        "                    train_outputs_onehot, test_outputs_onehot,\n",
        "                    create_cnn_model_one, best_params_one, 10,\n",
        "                    train_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wlmyDJC3YgF"
      },
      "source": [
        "---\n",
        "**Model 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyWeE3C-3YgG"
      },
      "source": [
        "> Next, let's run a grid search on the model to determine the best parameters for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgjvfG-M3YgH"
      },
      "source": [
        "# Define the model\n",
        "classifier = KerasClassifier(build_fn=create_cnn_model_two, verbose=0)\n",
        "\n",
        "# Define the parameters\n",
        "width = [train_images[0].shape[0]]\n",
        "height = [train_images[0].shape[1]]\n",
        "depth = [train_images[0].shape[2]]\n",
        "learning_rates = [0.001]\n",
        "activations = [ReLU, Softmax]\n",
        "optimizers = [Adagrad, Adam]\n",
        "\n",
        "# Turn it into a dictionary for the grid\n",
        "grid = dict(width = width,\n",
        "            height = height,\n",
        "            depth = depth,\n",
        "            learning_rate = learning_rates,\n",
        "            activation = activations,\n",
        "            optimizer = optimizers)\n",
        "\n",
        "# Run the grid search\n",
        "best_score_two, best_params_two, grid_two = model_grid_search(classifier, grid, \n",
        "                                             train_images, train_outputs_onehot)\n",
        "print(\"Best score: \" + str(best_score_two))\n",
        "print(\"Best parameters: \" + str(best_params_two))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFuEY21U3YgM"
      },
      "source": [
        "> Next, let's see what the best model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3leuh0wz3YgM"
      },
      "source": [
        "# Initialize the best model\n",
        "best_params_two = {'width': train_images[0].shape[0], 'height': train_images[0].shape[1], 'depth': train_images[0].shape[2],\n",
        "                   'learning_rate': 0.01, 'activation': ReLU, 'optimizer': Adagrad}\n",
        "model_two = create_cnn_model_two(**best_params_two)\n",
        "model_two.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "766HDz173YgR"
      },
      "source": [
        "> Finally, let's evaluate the optimized model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFjt8Skd3YgS"
      },
      "source": [
        "validate_tent_model(train_images, test_images, \n",
        "                    train_outputs_onehot, test_outputs_onehot,\n",
        "                    create_cnn_model_two, best_params_two, 10,\n",
        "                    train_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDH_A1YT3YgW"
      },
      "source": [
        "---\n",
        "**Model 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqy1_QwA3YgW"
      },
      "source": [
        "> Next, let's run a grid search on the model to determine the best parameters for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM9Ew8zu3YgY"
      },
      "source": [
        "# Define the model\n",
        "classifier = KerasClassifier(build_fn=create_cnn_model_three, verbose=0)\n",
        "\n",
        "# Define the parameters\n",
        "width = [train_images[0].shape[0]]\n",
        "height = [train_images[0].shape[1]]\n",
        "depth = [train_images[0].shape[2]]\n",
        "learning_rates = [0.001]\n",
        "activations = [ReLU, Softmax]\n",
        "optimizers = [Adagrad, Adam]\n",
        "\n",
        "# Turn it into a dictionary for the grid\n",
        "grid = dict(width = width,\n",
        "            height = height,\n",
        "            depth = depth,\n",
        "            learning_rate = learning_rates,\n",
        "            activation = activations,\n",
        "            optimizer = optimizers)\n",
        "\n",
        "# Run the grid search\n",
        "best_score_three, best_params_three, grid_three = model_grid_search(classifier, grid, \n",
        "                                             train_images, train_outputs_onehot)\n",
        "print(\"Best score: \" + str(best_score_three))\n",
        "print(\"Best parameters: \" + str(best_params_three))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOLj2ejq3Yga"
      },
      "source": [
        "> Next, let's see what the best model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuQMRlcV3Ygb"
      },
      "source": [
        "# Initialize the best model\n",
        "best_params_one = {'width': train_images[0].shape[0], 'height': train_images[0].shape[1], 'depth': train_images[0].shape[2],\n",
        "                   'learning_rate': 0.01, 'activation': ReLU, 'optimizer': Adam}\n",
        "model_three = create_cnn_model_three(**best_params_three)\n",
        "model_three.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zANl2kNC3Ygf"
      },
      "source": [
        "> Finally, let's evaluate the optimized model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDQ--ADT3Ygg"
      },
      "source": [
        "validate_tent_model(train_images, test_images, \n",
        "                    train_outputs_onehot, test_outputs_onehot,\n",
        "                    create_cnn_model_three, best_params_three, 10,\n",
        "                    train_classes)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}